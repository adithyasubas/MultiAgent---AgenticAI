name: Evaluation Suite

on:
  push:
    branches: ["main", "master"]
    paths:
      - "app.py"
      - "utils/**"
      - "evals/**"
      - "requirements.txt"
      - ".github/workflows/evals.yml"
  pull_request:
    branches: ["main", "master"]
    paths:
      - "app.py"
      - "utils/**"
      - "evals/**"
      - "requirements.txt"

jobs:
  run-evals:
    runs-on: ubuntu-latest
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Configure cache directories
        run: |
          echo "HF_HOME=$RUNNER_TEMP/hf-cache" >> "$GITHUB_ENV"
          echo "MPLCONFIGDIR=$RUNNER_TEMP/mpl-config" >> "$GITHUB_ENV"
          echo "XDG_CACHE_HOME=$RUNNER_TEMP/xdg-cache" >> "$GITHUB_ENV"
          mkdir -p "$RUNNER_TEMP/hf-cache" "$RUNNER_TEMP/mpl-config" "$RUNNER_TEMP/xdg-cache"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Lightweight dev tools for evaluation metrics
          pip install rouge-score bert-score

      - name: Prime Hugging Face cache
        run: |
          python - <<'PY'
          from transformers import AutoTokenizer, AutoModel
          model_name = "roberta-large"
          AutoTokenizer.from_pretrained(model_name)
          AutoModel.from_pretrained(model_name)
          PY

      - name: Run evaluation suite (mock mode)
        run: |
          python evals/run_evals.py --mode mock

      - name: Upload evaluation report
        uses: actions/upload-artifact@v4
        with:
          name: eval-report
          path: evals/results/

      - name: Optional live evaluation
        if: env.OPENAI_API_KEY != ''
        run: |
          python evals/run_evals.py --mode live --output-dir evals/results/live

      - name: Upload live evaluation report
        if: env.OPENAI_API_KEY != ''
        uses: actions/upload-artifact@v4
        with:
          name: eval-report-live
          path: evals/results/live/
